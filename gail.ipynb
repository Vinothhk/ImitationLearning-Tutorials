{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d9ea39ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "# Environment\n",
    "env = gym.make(\"CartPole-v1\",render_mode='human')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "# Hyperparameters\n",
    "lr_actor = 3e-4\n",
    "lr_discriminator = 1e-3\n",
    "gamma = 0.99\n",
    "epsilon = 0.2\n",
    "batch_size = 64\n",
    "num_episodes = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c311cd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, action_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, state):\n",
    "        return self.fc(state)\n",
    "\n",
    "policy = Policy()\n",
    "optimizer_actor = optim.Adam(policy.parameters(), lr=lr_actor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3d45df72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, 64),  # Now expects state_dim + action_dim\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, state, action):\n",
    "        action_onehot = torch.zeros(action.shape[0], action_dim)  # Shape: [batch_size, 2]\n",
    "        action_onehot.scatter_(1, action.long(), 1)  # Convert action to one-hot\n",
    "        x = torch.cat([state, action_onehot], dim=-1)  # Shape: [batch_size, 4+2=6]\n",
    "        return self.fc(x)\n",
    "    \n",
    "discriminator = Discriminator()\n",
    "optimizer_discriminator = optim.Adam(discriminator.parameters(), lr=lr_discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6007c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_expert_data(num_trajectories=50):\n",
    "    expert_states, expert_actions = [], []\n",
    "    for _ in range(num_trajectories):\n",
    "        state = env.reset()[0]\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = 1 if state[2] > 0 else 0  # Biased policy (balance pole)\n",
    "            expert_states.append(state)\n",
    "            expert_actions.append(action)\n",
    "            state, _, done,_ , _ = env.step(action)\n",
    "    return np.array(expert_states), np.array(expert_actions)\n",
    "\n",
    "expert_states, expert_actions = generate_expert_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0f70170",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def update_discriminator(expert_states, expert_actions, policy_states, policy_actions):\n",
    "    expert_states_tensor = torch.FloatTensor(expert_states)\n",
    "    expert_actions_tensor = torch.FloatTensor(expert_actions).unsqueeze(1)\n",
    "    policy_states_tensor = torch.FloatTensor(policy_states)\n",
    "    policy_actions_tensor = torch.FloatTensor(policy_actions).unsqueeze(1)\n",
    "    \n",
    "    # Discriminator loss\n",
    "    expert_probs = discriminator(expert_states_tensor, expert_actions_tensor)\n",
    "    policy_probs = discriminator(policy_states_tensor, policy_actions_tensor)\n",
    "    loss_d = -torch.mean(torch.log(expert_probs + 1e-8)) - torch.mean(torch.log(1 - policy_probs + 1e-8))\n",
    "    \n",
    "    optimizer_discriminator.zero_grad()\n",
    "    loss_d.backward()\n",
    "    optimizer_discriminator.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d23ef7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_advantages(rewards):\n",
    "    # Simplified advantage calculation (GAIL typically uses GAE)\n",
    "    return (rewards - np.mean(rewards)) / (np.std(rewards) + 1e-8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a82b69f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def update_policy(states, actions, advantages):\n",
    "    states_tensor = torch.FloatTensor(np.array(states))\n",
    "    actions_tensor = torch.LongTensor(np.array(actions))\n",
    "    advantages_tensor = torch.FloatTensor(advantages)\n",
    "    \n",
    "    # PPO loss\n",
    "    old_probs = policy(states_tensor).gather(1, actions_tensor.unsqueeze(1))\n",
    "    for _ in range(3):  # PPO epochs\n",
    "        new_probs = policy(states_tensor).gather(1, actions_tensor.unsqueeze(1))\n",
    "        ratio = new_probs / old_probs.detach()\n",
    "        surrogate_loss = -torch.min(\n",
    "            ratio * advantages_tensor.unsqueeze(1),\n",
    "            torch.clamp(ratio, 1 - epsilon, 1 + epsilon) * advantages_tensor.unsqueeze(1)\n",
    "        ).mean()\n",
    "        \n",
    "        optimizer_actor.zero_grad()\n",
    "        surrogate_loss.backward()\n",
    "        optimizer_actor.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b8bf00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gail():\n",
    "    for episode in range(num_episodes):\n",
    "        # Collect policy trajectories\n",
    "        states, actions, rewards = [], [], []\n",
    "        state = env.reset()\n",
    "        state = state[0]\n",
    "        done = False\n",
    "        while not done:\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            action_probs = policy(state_tensor)\n",
    "            action = torch.multinomial(action_probs, 1).item()\n",
    "            \n",
    "            next_state, _, done, _, _ = env.step(action)\n",
    "            \n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(0.0)  # Placeholder (updated later)\n",
    "            state = next_state\n",
    "        \n",
    "        # Convert to tensors\n",
    "        states_tensor = torch.FloatTensor(np.array(states))\n",
    "        actions_tensor = torch.FloatTensor(np.array(actions)).unsqueeze(1)\n",
    "        \n",
    "        # Compute rewards using discriminator\n",
    "        with torch.no_grad():\n",
    "            policy_rewards = torch.log(discriminator(states_tensor, actions_tensor) + 1e-8)\n",
    "        rewards = policy_rewards.squeeze().numpy()\n",
    "        \n",
    "        # Update policy (PPO)\n",
    "        advantages = compute_advantages(rewards)  # Simplified advantage calculation\n",
    "        update_policy(states, actions, advantages)\n",
    "        \n",
    "        # Update discriminator\n",
    "        update_discriminator(expert_states, expert_actions, states, actions)\n",
    "        \n",
    "        print(f\"Episode {episode}, Reward: {len(states)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eb9dd037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Reward: 17\n",
      "Episode 1, Reward: 68\n",
      "Episode 2, Reward: 25\n",
      "Episode 3, Reward: 23\n",
      "Episode 4, Reward: 30\n",
      "Episode 5, Reward: 30\n",
      "Episode 6, Reward: 28\n",
      "Episode 7, Reward: 12\n",
      "Episode 8, Reward: 30\n",
      "Episode 9, Reward: 18\n",
      "Episode 10, Reward: 17\n",
      "Episode 11, Reward: 14\n",
      "Episode 12, Reward: 10\n",
      "Episode 13, Reward: 20\n",
      "Episode 14, Reward: 47\n",
      "Episode 15, Reward: 13\n",
      "Episode 16, Reward: 11\n",
      "Episode 17, Reward: 26\n",
      "Episode 18, Reward: 12\n",
      "Episode 19, Reward: 19\n",
      "Episode 20, Reward: 9\n",
      "Episode 21, Reward: 9\n",
      "Episode 22, Reward: 13\n",
      "Episode 23, Reward: 29\n",
      "Episode 24, Reward: 24\n",
      "Episode 25, Reward: 14\n",
      "Episode 26, Reward: 30\n",
      "Episode 27, Reward: 14\n",
      "Episode 28, Reward: 13\n",
      "Episode 29, Reward: 16\n",
      "Episode 30, Reward: 11\n",
      "Episode 31, Reward: 17\n",
      "Episode 32, Reward: 18\n",
      "Episode 33, Reward: 19\n",
      "Episode 34, Reward: 16\n",
      "Episode 35, Reward: 17\n",
      "Episode 36, Reward: 14\n",
      "Episode 37, Reward: 10\n",
      "Episode 38, Reward: 13\n",
      "Episode 39, Reward: 12\n",
      "Episode 40, Reward: 25\n",
      "Episode 41, Reward: 30\n",
      "Episode 42, Reward: 32\n",
      "Episode 43, Reward: 12\n",
      "Episode 44, Reward: 15\n",
      "Episode 45, Reward: 22\n",
      "Episode 46, Reward: 11\n",
      "Episode 47, Reward: 15\n",
      "Episode 48, Reward: 31\n",
      "Episode 49, Reward: 13\n",
      "Episode 50, Reward: 64\n",
      "Episode 51, Reward: 19\n",
      "Episode 52, Reward: 14\n",
      "Episode 53, Reward: 13\n",
      "Episode 54, Reward: 36\n",
      "Episode 55, Reward: 20\n",
      "Episode 56, Reward: 21\n",
      "Episode 57, Reward: 22\n",
      "Episode 58, Reward: 19\n",
      "Episode 59, Reward: 11\n",
      "Episode 60, Reward: 57\n",
      "Episode 61, Reward: 13\n",
      "Episode 62, Reward: 13\n",
      "Episode 63, Reward: 45\n",
      "Episode 64, Reward: 15\n",
      "Episode 65, Reward: 12\n",
      "Episode 66, Reward: 31\n",
      "Episode 67, Reward: 21\n",
      "Episode 68, Reward: 40\n",
      "Episode 69, Reward: 39\n",
      "Episode 70, Reward: 20\n",
      "Episode 71, Reward: 48\n",
      "Episode 72, Reward: 30\n",
      "Episode 73, Reward: 26\n",
      "Episode 74, Reward: 42\n",
      "Episode 75, Reward: 12\n",
      "Episode 76, Reward: 25\n",
      "Episode 77, Reward: 99\n",
      "Episode 78, Reward: 24\n",
      "Episode 79, Reward: 32\n",
      "Episode 80, Reward: 55\n",
      "Episode 81, Reward: 23\n",
      "Episode 82, Reward: 16\n",
      "Episode 83, Reward: 46\n",
      "Episode 84, Reward: 25\n",
      "Episode 85, Reward: 32\n",
      "Episode 86, Reward: 15\n",
      "Episode 87, Reward: 24\n",
      "Episode 88, Reward: 14\n",
      "Episode 89, Reward: 22\n",
      "Episode 90, Reward: 35\n",
      "Episode 91, Reward: 38\n",
      "Episode 92, Reward: 24\n",
      "Episode 93, Reward: 37\n",
      "Episode 94, Reward: 19\n",
      "Episode 95, Reward: 26\n",
      "Episode 96, Reward: 33\n",
      "Episode 97, Reward: 33\n",
      "Episode 98, Reward: 88\n",
      "Episode 99, Reward: 36\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Start training\n",
    "train_gail()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "act_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
