{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76fdcbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import os\n",
    "import pickle\n",
    "from stable_baselines3 import PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d524d0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fda768b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space: ()\n",
      "Observation Space: 4\n"
     ]
    }
   ],
   "source": [
    "print(\"Action Space:\", env.action_space.shape)\n",
    "print(\"Observation Space:\", env.observation_space.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f095ee03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "950905bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff64e8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE_SIZE = env.observation_space.shape[0]  # Number of features in the state\n",
    "ACTION_SIZE = 2\n",
    "\n",
    "#\n",
    "# Define model\n",
    "#\n",
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(STATE_SIZE, hidden_dim)\n",
    "        self.classify = nn.Linear(hidden_dim, ACTION_SIZE)\n",
    "\n",
    "    def forward(self, s):\n",
    "        outs = self.hidden(s)\n",
    "        outs = F.relu(outs)\n",
    "        logits = self.classify(outs)\n",
    "        return logits\n",
    "\n",
    "#\n",
    "# Generate model\n",
    "#\n",
    "policy_func = PolicyNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b84c19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_sample_and_logits(policy, s):\n",
    "    \"\"\"\n",
    "    Stochastically pick up action and logits with policy model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    policy : torch.nn.Module\n",
    "        Policy network to use\n",
    "    s : torch.tensor((..., STATE_SIZE), dtype=int)\n",
    "        The feature (one-hot) of state.\n",
    "        The above \"...\" can have arbitrary shape with 0 or 1 dimension.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    action : torch.tensor((...), dtype=int)\n",
    "        The picked-up actions.\n",
    "        If input shape is (*, STATE_SIZE), this shape becomes (*).\n",
    "    logits : torch.tensor((..., ACTION_SIZE), dtype=float)\n",
    "        Logits of categorical distribution (used to optimize model).\n",
    "        If input shape is (*, STATE_SIZE), this shape becomes (*, ACTION_SIZE).\n",
    "    \"\"\"\n",
    "    # get logits from state\n",
    "    # --> size : (*, ACTION_SIZE)\n",
    "    logits = policy(s.float())\n",
    "    # from logits to probabilities\n",
    "    # --> size : (*, ACTION_SIZE)\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    # pick up action's sample\n",
    "    # --> size : (*, 1)\n",
    "    a = torch.multinomial(probs, num_samples=1)\n",
    "    # --> size : (*)\n",
    "    a = a.squeeze()\n",
    "\n",
    "    # Return\n",
    "    return a, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6347cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated rewards (before training): 0.03666666666666667\n"
     ]
    }
   ],
   "source": [
    "def evaluate(policy, batch_size):\n",
    "    total_reward = torch.tensor(0.0).to(device)\n",
    "    s = env.reset()\n",
    "    if isinstance(s, tuple):  # Gymnasium returns (obs, info)\n",
    "        s = s[0]\n",
    "    while True:\n",
    "        s_tensor = torch.tensor(s, dtype=torch.float32).to(device)\n",
    "        with torch.no_grad():\n",
    "            a, _ = pick_sample_and_logits(policy, s_tensor)\n",
    "        s, r, term, trunc, _ = env.step(a.item())\n",
    "        total_reward += r\n",
    "        done = term or trunc\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward.item() / batch_size\n",
    "\n",
    "avg_reward = evaluate(policy_func, 300)\n",
    "print(f\"Estimated rewards (before training): {avg_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e5db24",
   "metadata": {},
   "source": [
    "# Save Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0097e7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.makedirs(\"./expert_data\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ad86533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expert dataset saved to ./expert_data/ckpt0.pkl\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "expert_model = PPO.load(\"models/ppo_cartpole_expert\")  # adjust path if needed\n",
    "\n",
    "num_episodes = 1000\n",
    "all_states = []\n",
    "all_actions = []\n",
    "timestep_lens = []\n",
    "\n",
    "for ep in range(num_episodes):\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    states = []\n",
    "    actions = []\n",
    "    while not done:\n",
    "        states.append(obs)\n",
    "        action, _ = expert_model.predict(obs, deterministic=True)\n",
    "        actions.append(action)\n",
    "        obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "    all_states.extend(states)\n",
    "    all_actions.extend(actions)\n",
    "    timestep_lens.append(len(states))\n",
    "\n",
    "# Save expert data\n",
    "expert_data = {\n",
    "    \"states\": all_states,\n",
    "    \"actions\": all_actions,\n",
    "    \"timestep_lens\": timestep_lens\n",
    "}\n",
    "\n",
    "with open(\"./expert_data/ckpt0.pkl\", \"wb\") as f:\n",
    "    pickle.dump(expert_data, f)\n",
    "\n",
    "print(\"Expert dataset saved to ./expert_data/ckpt0.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9101ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed   100 episodes in checkpoint ckpt0.pkl...\n",
      "Evaluation result (Average reward): 0.09\n",
      "Processed   200 episodes in checkpoint ckpt0.pkl...\n",
      "Evaluation result (Average reward): 0.405\n",
      "Processed   300 episodes in checkpoint ckpt0.pkl...\n",
      "Evaluation result (Average reward): 0.31\n",
      "Processed   400 episodes in checkpoint ckpt0.pkl...\n",
      "Evaluation result (Average reward): 0.39\n",
      "Processed   500 episodes in checkpoint ckpt0.pkl...\n",
      "Evaluation result (Average reward): 0.42\n",
      "Processed   600 episodes in checkpoint ckpt0.pkl...\n",
      "Evaluation result (Average reward): 0.465\n",
      "Processed   700 episodes in checkpoint ckpt0.pkl...\n",
      "Evaluation result (Average reward): 0.44\n",
      "Processed   800 episodes in checkpoint ckpt0.pkl...\n",
      "Evaluation result (Average reward): 0.445\n",
      "Processed   900 episodes in checkpoint ckpt0.pkl...\n",
      "Evaluation result (Average reward): 1.555\n",
      "Processed  1000 episodes in checkpoint ckpt0.pkl...\n",
      "Evaluation result (Average reward): 1.005\n"
     ]
    }
   ],
   "source": [
    "# use the following expert dataset\n",
    "dest_dir = \"./expert_data\"\n",
    "checkpoint_files = [\"ckpt0.pkl\"]\n",
    "\n",
    "# create optimizer\n",
    "opt = torch.optim.AdamW(policy_func.parameters(), lr=0.001)\n",
    "\n",
    "for ckpt in checkpoint_files:\n",
    "    # load expert data from pickle\n",
    "    with open(f\"{dest_dir}/{ckpt}\", \"rb\") as f:\n",
    "        all_data = pickle.load(f)\n",
    "    all_states = all_data[\"states\"]\n",
    "    all_actions = all_data[\"actions\"]\n",
    "    timestep_lens = all_data[\"timestep_lens\"]\n",
    "    # loop all episodes in demonstration\n",
    "    current_timestep = 0\n",
    "    for i, timestep_len in enumerate(timestep_lens):\n",
    "        # pick up states and actions in a single episode\n",
    "        states = all_states[current_timestep:current_timestep+timestep_len]\n",
    "        actions = all_actions[current_timestep:current_timestep+timestep_len]\n",
    "        # collect loss and optimize (train)\n",
    "        opt.zero_grad()\n",
    "        loss = []\n",
    "        # for s, a in zip(states, actions):\n",
    "        #     s_tensor = torch.tensor(s, dtype=torch.float32).to(device)  # Use float for CartPole\n",
    "        #     _, logits = pick_sample_and_logits(policy_func, s_tensor)\n",
    "        #     logits = logits.unsqueeze(0)\n",
    "        #     print(f\"Logits shape: {logits.shape}, Action: {a}\")\n",
    "        #     print(f\"Logits: {logits}, Action: {a}\")\n",
    "        #     l = F.cross_entropy(logits, torch.tensor([a]).to(device), reduction=\"none\")\n",
    "        #     loss.append(l)\n",
    "        for s, a in zip(states, actions):\n",
    "            s_tensor = torch.tensor(s, dtype=torch.float32).to(device)\n",
    "            _, logits = pick_sample_and_logits(policy_func, s_tensor)\n",
    "            logits = logits.unsqueeze(0)  # shape: [1, ACTION_SIZE]\n",
    "            \n",
    "            a = int(a)  # ✅ convert to native Python int\n",
    "            target = torch.tensor([a], dtype=torch.long, device=device)  # shape: [1]\n",
    "            \n",
    "            l = F.cross_entropy(logits, target, reduction=\"none\")\n",
    "            loss.append(l)\n",
    "        total_loss = torch.stack(loss, dim=0)\n",
    "        total_loss.sum().backward()\n",
    "        opt.step()\n",
    "        # log\n",
    "        print(\"Processed {:5d} episodes in checkpoint {}...\".format(i + 1, ckpt), end=\"\\r\")\n",
    "        # run evaluation in each 1000 episodes\n",
    "        if i % 100 == 99:\n",
    "            avg = evaluate(policy_func, 200)\n",
    "            print(f\"\\nEvaluation result (Average reward): {avg}\")\n",
    "        # proceed to next episode\n",
    "        current_timestep += timestep_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4db0c83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_func = policy_func.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf818d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward: 175.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "obs, _ = env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "\n",
    "while not done:\n",
    "    s_tensor = torch.tensor(obs, dtype=torch.float32).to(device)\n",
    "    with torch.no_grad():\n",
    "        action, _ = pick_sample_and_logits(policy_func, s_tensor)\n",
    "    obs, reward, terminated, truncated, _ = env.step(action.item())\n",
    "    total_reward += reward\n",
    "    done = terminated or truncated\n",
    "\n",
    "print(f\"Total reward: {total_reward}\")\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "act_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
