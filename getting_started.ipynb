{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a94adcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "\n",
    "from imitation.algorithms import bc\n",
    "from imitation.data import rollout\n",
    "from imitation.data.wrappers import RolloutInfoWrapper\n",
    "from imitation.policies.serialize import load_policy\n",
    "from imitation.util.util import make_vec_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf2c2d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(0)\n",
    "env = make_vec_env(\n",
    "    \"seals:seals/CartPole-v0\",\n",
    "    rng=rng,\n",
    "    post_wrappers=[lambda env, _: RolloutInfoWrapper(env)],  # for computing rollouts\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90e639b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_expert():\n",
    "    # note: use `download_expert` instead to download a pretrained, competent expert\n",
    "    print(\"Training a expert.\")\n",
    "    expert = PPO(\n",
    "        policy=MlpPolicy,\n",
    "        env=env,\n",
    "        seed=0,\n",
    "        batch_size=64,\n",
    "        ent_coef=0.0,\n",
    "        learning_rate=0.0003,\n",
    "        n_epochs=10,\n",
    "        n_steps=64,\n",
    "    )\n",
    "    expert.learn(1_000)  # Note: change this to 100_000 to train a decent expert.\n",
    "    return expert\n",
    "\n",
    "\n",
    "def download_expert():\n",
    "    print(\"Downloading a pretrained expert.\")\n",
    "    expert = load_policy(\n",
    "        \"ppo-huggingface\",\n",
    "        organization=\"HumanCompatibleAI\",\n",
    "        env_name=\"seals-CartPole-v0\",\n",
    "        venv=env,\n",
    "    )\n",
    "    return expert\n",
    "\n",
    "\n",
    "def sample_expert_transitions():\n",
    "    # expert = train_expert()  # uncomment to train your own expert\n",
    "    expert = download_expert()\n",
    "\n",
    "    print(\"Sampling expert transitions.\")\n",
    "    rollouts = rollout.rollout(\n",
    "        expert,\n",
    "        env,\n",
    "        rollout.make_sample_until(min_timesteps=None, min_episodes=50),\n",
    "        rng=rng,\n",
    "    )\n",
    "    return rollout.flatten_trajectories(rollouts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2e29da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading a pretrained expert.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e9e8331c6d14e5e9360ee22aa67dfa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ppo-seals-CartPole-v0.zip:   0%|          | 0.00/139k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling expert transitions.\n",
      "Evaluating the untrained policy.\n",
      "Reward before training: 103.0\n",
      "Training a policy using Behavior Cloning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0batch [00:00, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| batch_size        | 32        |\n",
      "| bc/               |           |\n",
      "|    batch          | 0         |\n",
      "|    ent_loss       | -0.000693 |\n",
      "|    entropy        | 0.693     |\n",
      "|    epoch          | 0         |\n",
      "|    l2_loss        | 0         |\n",
      "|    l2_norm        | 72.5      |\n",
      "|    loss           | 0.692     |\n",
      "|    neglogp        | 0.693     |\n",
      "|    prob_true_act  | 0.5       |\n",
      "|    samples_so_far | 32        |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "475batch [00:01, 424.09batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| batch_size        | 32        |\n",
      "| bc/               |           |\n",
      "|    batch          | 500       |\n",
      "|    ent_loss       | -0.000231 |\n",
      "|    entropy        | 0.231     |\n",
      "|    epoch          | 0         |\n",
      "|    l2_loss        | 0         |\n",
      "|    l2_norm        | 95.3      |\n",
      "|    loss           | 0.159     |\n",
      "|    neglogp        | 0.16      |\n",
      "|    prob_true_act  | 0.888     |\n",
      "|    samples_so_far | 16032     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "875batch [00:02, 395.53batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the trained policy.\n",
      "Reward after training: 500.0\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "transitions = sample_expert_transitions()\n",
    "bc_trainer = bc.BC(\n",
    "    observation_space=env.observation_space,\n",
    "    action_space=env.action_space,\n",
    "    demonstrations=transitions,\n",
    "    rng=rng,\n",
    ")\n",
    "\n",
    "evaluation_env = make_vec_env(\n",
    "    \"seals:seals/CartPole-v0\",\n",
    "    rng=rng,\n",
    "    env_make_kwargs={\"render_mode\": \"human\"},  # for rendering\n",
    ")\n",
    "\n",
    "print(\"Evaluating the untrained policy.\")\n",
    "reward, _ = evaluate_policy(\n",
    "    bc_trainer.policy,  # type: ignore[arg-type]\n",
    "    evaluation_env,\n",
    "    n_eval_episodes=3,\n",
    "    render=True,  # comment out to speed up\n",
    ")\n",
    "print(f\"Reward before training: {reward}\")\n",
    "\n",
    "print(\"Training a policy using Behavior Cloning\")\n",
    "bc_trainer.train(n_epochs=1)\n",
    "\n",
    "print(\"Evaluating the trained policy.\")\n",
    "reward, _ = evaluate_policy(\n",
    "    bc_trainer.policy,  # type: ignore[arg-type]\n",
    "    evaluation_env,\n",
    "    n_eval_episodes=3,\n",
    "    render=True,  # comment out to speed up\n",
    ")\n",
    "print(f\"Reward after training: {reward}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "act_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
