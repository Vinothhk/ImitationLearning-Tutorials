{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d26ea974",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from itertools import product\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a166c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# 1. Environment and Data Setup (FIXED)\n",
    "# =============================================\n",
    "\n",
    "class CartPoleWrapper:\n",
    "    def __init__(self, render=False):\n",
    "        self.render = render\n",
    "        self.env = gym.make(\"CartPole-v1\", render_mode=\"human\" if render else None)\n",
    "        self.state_dim = 4\n",
    "        self.action_dim = 2\n",
    "        self.discrete_actions = [0, 1]\n",
    "        \n",
    "    def reset(self):\n",
    "        return self.env.reset()[0]\n",
    "    \n",
    "    def step(self, action):\n",
    "        state, reward, done, _, info = self.env.step(action)\n",
    "        return state, reward, done, info\n",
    "    \n",
    "    def get_features(self, state):\n",
    "        return np.array([\n",
    "            state[0],               # Cart position\n",
    "            state[1],               # Cart velocity\n",
    "            state[2],               # Pole angle\n",
    "            state[3],               # Pole angular velocity\n",
    "            state[0] * state[2],    # Position-angle interaction\n",
    "            state[1] * state[3]     # Velocity-angular velocity interaction\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97e7b962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# 2. Feature Calculations (FIXED)\n",
    "# =============================================\n",
    "\n",
    "def compute_expert_feature_expectations(expert_states, gamma=0.99):\n",
    "    feature_dim = len(CartPoleWrapper().get_features(expert_states[0][0]))\n",
    "    mu_E = np.zeros(feature_dim)\n",
    "\n",
    "    for traj in expert_states:\n",
    "        weight = 1.0\n",
    "        for state in traj:\n",
    "            mu_E += weight * CartPoleWrapper().get_features(state)\n",
    "            weight *= gamma\n",
    "\n",
    "    return mu_E / len(expert_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ceaef85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# 3. Soft Value Iteration (FIXED)\n",
    "# =============================================\n",
    "def soft_value_iteration(w, env_wrapper, gamma=0.99, n_iters=100, n_bins=20):\n",
    "    env = env_wrapper.env\n",
    "    s_space = [\n",
    "        np.linspace(-4.8, 4.8, n_bins),\n",
    "        np.linspace(-3.4, 3.4, n_bins),\n",
    "        np.linspace(-0.418, 0.418, n_bins),\n",
    "        np.linspace(-2.0, 2.0, n_bins)\n",
    "    ]\n",
    "    \n",
    "    V = np.zeros(tuple([n_bins] * env_wrapper.state_dim))\n",
    "\n",
    "    for _ in range(n_iters):\n",
    "        new_V = np.zeros_like(V)\n",
    "        for idx in product(*[range(n_bins) for _ in range(env_wrapper.state_dim)]):\n",
    "            s_cont = np.array([s_space[d][idx[d]] for d in range(env_wrapper.state_dim)])\n",
    "            Q_values = []\n",
    "            for a in env_wrapper.discrete_actions:\n",
    "                original_state = env.env.state\n",
    "                if env_wrapper.render:\n",
    "                    env.reset()\n",
    "                env.env.state = s_cont\n",
    "                s_next, _, done, _, _ = env.step(a)\n",
    "                s_next = np.array(s_next)\n",
    "                s_next_idx = tuple(\n",
    "                    np.clip(np.digitize(s_next[i], s_space[i]) - 1, 0, n_bins - 1)\n",
    "                    for i in range(env_wrapper.state_dim)\n",
    "                )\n",
    "                cost = np.dot(w, env_wrapper.get_features(s_cont))\n",
    "                Q = cost if done else cost + gamma * V[s_next_idx]\n",
    "                Q_values.append(Q)\n",
    "                env.env.state = original_state\n",
    "            new_V[idx] = np.log(np.sum(np.exp(Q_values)))\n",
    "        V = new_V\n",
    "\n",
    "    return V, s_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57573b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# 4. Learner Feature Expectations (FIXED)\n",
    "# =============================================\n",
    "\n",
    "def compute_learner_feature_expectations(w, env_wrapper, V, s_space, \n",
    "                                         gamma=0.99, n_trajs=10):\n",
    "    feature_dim = len(env_wrapper.get_features(env_wrapper.reset()))\n",
    "    mu_learner = np.zeros(feature_dim)\n",
    "\n",
    "    for _ in range(n_trajs):\n",
    "        state = env_wrapper.reset()\n",
    "        done = False\n",
    "        weight = 1.0\n",
    "\n",
    "        while not done:\n",
    "            state_idx = tuple(\n",
    "                np.clip(np.digitize(state[i], s_space[i]) - 1, 0, len(s_space[i]) - 1)\n",
    "                for i in range(env_wrapper.state_dim)\n",
    "            )\n",
    "            Q_values = []\n",
    "            for a in env_wrapper.discrete_actions:\n",
    "                original_state = env_wrapper.env.env.state\n",
    "                env_wrapper.env.env.state = state\n",
    "                if env_wrapper.render:\n",
    "                    env_wrapper.env.reset()\n",
    "                s_next, _, done, _ = env_wrapper.step(a)\n",
    "                s_next = np.array(s_next)\n",
    "                s_next_idx = tuple(\n",
    "                    np.clip(np.digitize(s_next[i], s_space[i]) - 1, 0, len(s_space[i]) - 1)\n",
    "                    for i in range(env_wrapper.state_dim)\n",
    "                )\n",
    "                cost = np.dot(w, env_wrapper.get_features(state))\n",
    "                Q = cost if done else cost + gamma * V[s_next_idx]\n",
    "                Q_values.append(Q)\n",
    "                env_wrapper.env.env.state = original_state\n",
    "            max_Q = np.max(Q_values)\n",
    "            policy = np.exp(Q_values - max_Q)\n",
    "            policy = policy / np.sum(policy)\n",
    "            action = np.random.choice(env_wrapper.discrete_actions, p=policy)\n",
    "            mu_learner += weight * env_wrapper.get_features(state)\n",
    "            state, _, done, _ = env_wrapper.step(action)\n",
    "            weight *= gamma\n",
    "\n",
    "    return mu_learner / n_trajs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6dffa80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# 5. MaxEnt IRL Main Loop (FIXED)\n",
    "# =============================================\n",
    "\n",
    "def maxent_irl(expert_states, env_wrapper, lr=0.1, n_irl_iters=50, \n",
    "               n_vi_iters=100, n_trajs=10, n_bins=20):\n",
    "    feature_dim = len(env_wrapper.get_features(env_wrapper.reset()))\n",
    "    w = np.random.randn(feature_dim) * 0.1\n",
    "    mu_E = compute_expert_feature_expectations(expert_states)\n",
    "    losses = []\n",
    "\n",
    "    for it in tqdm(range(n_irl_iters)):\n",
    "        V, s_space = soft_value_iteration(w, env_wrapper, n_iters=n_vi_iters, n_bins=n_bins)\n",
    "        mu_learner = compute_learner_feature_expectations(w, env_wrapper, V, s_space, n_trajs=n_trajs)\n",
    "        gradient = mu_E - mu_learner\n",
    "        w += lr * gradient\n",
    "        loss = np.linalg.norm(gradient)\n",
    "        losses.append(loss)\n",
    "        tqdm.write(f\"Iter {it+1}/{n_irl_iters}, Loss: {loss:.4f}\")\n",
    "    \n",
    "    return w, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e90be149",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/home/vinoth/HuggFace/act_env/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.state to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.state` for environment variables or `env.get_wrapper_attr('state')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/vinoth/HuggFace/act_env/lib/python3.10/site-packages/gymnasium/envs/classic_control/cartpole.py:180: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned terminated = True. You should always call 'reset()' once you receive 'terminated = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  logger.warn(\n",
      " 10%|█         | 1/10 [00:36<05:28, 36.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/10, Loss: 4.0877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [01:12<04:49, 36.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 2/10, Loss: 3.1016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [01:47<04:10, 35.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 3/10, Loss: 4.7907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [02:24<03:36, 36.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 4/10, Loss: 5.3009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [03:00<03:00, 36.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 5/10, Loss: 4.2242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [03:35<02:23, 35.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 6/10, Loss: 3.8395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [04:12<01:48, 36.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 7/10, Loss: 3.1224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [04:48<01:12, 36.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 8/10, Loss: 3.8025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [05:25<00:36, 36.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 9/10, Loss: 4.7877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [06:01<00:00, 36.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 10/10, Loss: 4.0550\n",
      "Learned weights: [-3.22282126 -1.53768332 -0.18691442 -0.14425328  0.00549319 -1.02069414]\n",
      "Losses: [np.float64(4.087674021724666), np.float64(3.101588865713964), np.float64(4.790657737092499), np.float64(5.300858085215364), np.float64(4.224191682578222), np.float64(3.8394848624636153), np.float64(3.1224014190494733), np.float64(3.8025145236611846), np.float64(4.787674889808097), np.float64(4.055021773748474)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "# 6. Executionvec and Expert Data Handling (FIXED)\n",
    "# =============================================\n",
    "\n",
    "env_wrapper = CartPoleWrapper(render=False)\n",
    "\n",
    "# Load expert data\n",
    "with open(\"./expert_data/ckpt0.pkl\", \"rb\") as f:\n",
    "    exp_data = pickle.load(f)\n",
    "exp_states = exp_data[\"states\"]\n",
    "timestep_lens = exp_data[\"timestep_lens\"]\n",
    "\n",
    "# Reconstruct expert state trajectories\n",
    "expert_states = []\n",
    "current = 0\n",
    "for length in timestep_lens:\n",
    "    episode = exp_states[current:current+length]\n",
    "    expert_states.append(np.array(episode))\n",
    "    current += length\n",
    "\n",
    "# Run MaxEnt IRL\n",
    "learned_weights, losses = maxent_irl(\n",
    "    expert_states,\n",
    "    env_wrapper,\n",
    "    lr=0.1,\n",
    "    n_irl_iters=10,   # Keep low for initial testing\n",
    "    n_vi_iters=50,\n",
    "    n_trajs=5,\n",
    "    n_bins=10\n",
    ")\n",
    "\n",
    "print(\"Learned weights:\", learned_weights)\n",
    "print(\"Losses:\", losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ea3caaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_policy(w, env_wrapper, V, s_space, gamma=0.99, n_episodes=5, render=False):\n",
    "    \"\"\"\n",
    "    Evaluate the learned policy by running it in the real environment.\n",
    "    \"\"\"\n",
    "    for ep in range(n_episodes):\n",
    "        state = env_wrapper.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "\n",
    "        while not done:\n",
    "            if render:\n",
    "                env_wrapper.env.render()  # show the environment\n",
    "\n",
    "            # Discretize state\n",
    "            state_idx = tuple(\n",
    "                np.clip(np.digitize(state[i], s_space[i]) - 1, 0, len(s_space[i]) - 1)\n",
    "                for i in range(env_wrapper.state_dim)\n",
    "            )\n",
    "\n",
    "            # Compute Q-values for both actions\n",
    "            Q_values = []\n",
    "            for a in env_wrapper.discrete_actions:\n",
    "                original_state = env_wrapper.env.env.state\n",
    "                env_wrapper.env.env.state = state\n",
    "                s_next, _, done, _ = env_wrapper.step(a)\n",
    "                s_next = np.array(s_next)\n",
    "\n",
    "                s_next_idx = tuple(\n",
    "                    np.clip(np.digitize(s_next[i], s_space[i]) - 1, 0, len(s_space[i]) - 1)\n",
    "                    for i in range(env_wrapper.state_dim)\n",
    "                )\n",
    "\n",
    "                cost = np.dot(w, env_wrapper.get_features(state))\n",
    "                if done:\n",
    "                    Q = cost\n",
    "                else:\n",
    "                    Q = cost + gamma * V[s_next_idx]\n",
    "                Q_values.append(Q)\n",
    "\n",
    "                env_wrapper.env.env.state = original_state\n",
    "\n",
    "            # Softmax policy\n",
    "            max_Q = np.max(Q_values)\n",
    "            policy = np.exp(Q_values - max_Q)\n",
    "            policy = policy / np.sum(policy)\n",
    "\n",
    "            action = np.random.choice(env_wrapper.discrete_actions, p=policy)\n",
    "\n",
    "            # Step\n",
    "            state, reward, done, _ = env_wrapper.step(action)\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "\n",
    "        print(f\"Episode {ep + 1}: Total reward = {total_reward}, Steps = {steps}\")\n",
    "\n",
    "    env_wrapper.env.close()  # close the rendering window after testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "29a6ffa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinoth/HuggFace/act_env/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.state to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.state` for environment variables or `env.get_wrapper_attr('state')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +=: 'NoneType' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m env_wrapper \u001b[38;5;241m=\u001b[39m CartPoleWrapper(render\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 2\u001b[0m V, s_space \u001b[38;5;241m=\u001b[39m \u001b[43msoft_value_iteration\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearned_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv_wrapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_iters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_bins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\n\u001b[1;32m      4\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m test_policy(learned_weights, env_wrapper, V, s_space, n_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, render\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[16], line 25\u001b[0m, in \u001b[0;36msoft_value_iteration\u001b[0;34m(w, env_wrapper, gamma, n_iters, n_bins)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m env_wrapper\u001b[38;5;241m.\u001b[39mrender:\n\u001b[1;32m     24\u001b[0m     env\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m---> 25\u001b[0m s_next, _, done, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m s_next \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(s_next)\n\u001b[1;32m     27\u001b[0m s_next_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[1;32m     28\u001b[0m     np\u001b[38;5;241m.\u001b[39mclip(np\u001b[38;5;241m.\u001b[39mdigitize(s_next[i], s_space[i]) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, n_bins \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(env_wrapper\u001b[38;5;241m.\u001b[39mstate_dim)\n\u001b[1;32m     30\u001b[0m )\n",
      "File \u001b[0;32m~/HuggFace/act_env/lib/python3.10/site-packages/gymnasium/wrappers/time_limit.py:58\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m \n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     57\u001b[0m observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m---> 58\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n\u001b[1;32m     61\u001b[0m     truncated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +=: 'NoneType' and 'int'"
     ]
    }
   ],
   "source": [
    "\n",
    "env_wrapper = CartPoleWrapper(render=True)\n",
    "V, s_space = soft_value_iteration(\n",
    "    learned_weights, env_wrapper, n_iters=50, n_bins=10\n",
    ")\n",
    "test_policy(learned_weights, env_wrapper, V, s_space, n_episodes=5, render=True)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "act_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
