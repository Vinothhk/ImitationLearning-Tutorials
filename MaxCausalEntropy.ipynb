{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d26ea974",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from itertools import product\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5a166c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================\n",
    "# 1. Environment and Data Setup (FIXED)\n",
    "# =============================================\n",
    "\n",
    "class CartPoleWrapper:\n",
    "    def __init__(self):\n",
    "        self.env = gym.make('CartPole-v1')\n",
    "        self.state_dim = 4\n",
    "        self.action_dim = 2\n",
    "        self.discrete_actions = [0, 1]\n",
    "        \n",
    "    def reset(self):\n",
    "        return self.env.reset()[0]\n",
    "    \n",
    "    def step(self, action):\n",
    "        state, reward, done, _, info = self.env.step(action)\n",
    "        return state, reward, done, info\n",
    "    \n",
    "    def get_features(self, state):\n",
    "        return np.array([\n",
    "            state[0],               # Cart position\n",
    "            state[1],               # Cart velocity\n",
    "            state[2],               # Pole angle\n",
    "            state[3],               # Pole angular velocity\n",
    "            state[0] * state[2],    # Position-angle interaction\n",
    "            state[1] * state[3]     # Velocity-angular velocity interaction\n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "97e7b962",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================\n",
    "# 2. Feature Calculations (FIXED)\n",
    "# =============================================\n",
    "\n",
    "def compute_expert_feature_expectations(expert_states, gamma=0.99):\n",
    "    feature_dim = len(CartPoleWrapper().get_features(expert_states[0][0]))\n",
    "    mu_E = np.zeros(feature_dim)\n",
    "\n",
    "    for traj in expert_states:\n",
    "        weight = 1.0\n",
    "        for state in traj:\n",
    "            mu_E += weight * CartPoleWrapper().get_features(state)\n",
    "            weight *= gamma\n",
    "\n",
    "    return mu_E / len(expert_states)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4ceaef85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================\n",
    "# 3. Soft Value Iteration (FIXED)\n",
    "# =============================================\n",
    "def soft_value_iteration(w, env_wrapper, gamma=0.99, n_iters=100, n_bins=20):\n",
    "    env = env_wrapper.env\n",
    "    s_space = [\n",
    "        np.linspace(-4.8, 4.8, n_bins),\n",
    "        np.linspace(-3.4, 3.4, n_bins),\n",
    "        np.linspace(-0.418, 0.418, n_bins),\n",
    "        np.linspace(-2.0, 2.0, n_bins)\n",
    "    ]\n",
    "    \n",
    "    V = np.zeros(tuple([n_bins] * env_wrapper.state_dim))\n",
    "\n",
    "    for _ in range(n_iters):\n",
    "        new_V = np.zeros_like(V)\n",
    "        for idx in product(*[range(n_bins) for _ in range(env_wrapper.state_dim)]):\n",
    "            s_cont = np.array([s_space[d][idx[d]] for d in range(env_wrapper.state_dim)])\n",
    "            Q_values = []\n",
    "            for a in env_wrapper.discrete_actions:\n",
    "                original_state = env.env.state\n",
    "                env.env.state = s_cont\n",
    "                s_next, _, done, _, _ = env.step(a)\n",
    "                s_next = np.array(s_next)\n",
    "                s_next_idx = tuple(\n",
    "                    np.clip(np.digitize(s_next[i], s_space[i]) - 1, 0, n_bins - 1)\n",
    "                    for i in range(env_wrapper.state_dim)\n",
    "                )\n",
    "                cost = np.dot(w, env_wrapper.get_features(s_cont))\n",
    "                Q = cost if done else cost + gamma * V[s_next_idx]\n",
    "                Q_values.append(Q)\n",
    "                env.env.state = original_state\n",
    "            new_V[idx] = np.log(np.sum(np.exp(Q_values)))\n",
    "        V = new_V\n",
    "\n",
    "    return V, s_space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "57573b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================\n",
    "# 4. Learner Feature Expectations (FIXED)\n",
    "# =============================================\n",
    "\n",
    "def compute_learner_feature_expectations(w, env_wrapper, V, s_space, \n",
    "                                         gamma=0.99, n_trajs=10):\n",
    "    feature_dim = len(env_wrapper.get_features(env_wrapper.reset()))\n",
    "    mu_learner = np.zeros(feature_dim)\n",
    "\n",
    "    for _ in range(n_trajs):\n",
    "        state = env_wrapper.reset()\n",
    "        done = False\n",
    "        weight = 1.0\n",
    "\n",
    "        while not done:\n",
    "            state_idx = tuple(\n",
    "                np.clip(np.digitize(state[i], s_space[i]) - 1, 0, len(s_space[i]) - 1)\n",
    "                for i in range(env_wrapper.state_dim)\n",
    "            )\n",
    "            Q_values = []\n",
    "            for a in env_wrapper.discrete_actions:\n",
    "                original_state = env_wrapper.env.env.state\n",
    "                env_wrapper.env.env.state = state\n",
    "                s_next, _, done, _ = env_wrapper.step(a)\n",
    "                s_next = np.array(s_next)\n",
    "                s_next_idx = tuple(\n",
    "                    np.clip(np.digitize(s_next[i], s_space[i]) - 1, 0, len(s_space[i]) - 1)\n",
    "                    for i in range(env_wrapper.state_dim)\n",
    "                )\n",
    "                cost = np.dot(w, env_wrapper.get_features(state))\n",
    "                Q = cost if done else cost + gamma * V[s_next_idx]\n",
    "                Q_values.append(Q)\n",
    "                env_wrapper.env.env.state = original_state\n",
    "            max_Q = np.max(Q_values)\n",
    "            policy = np.exp(Q_values - max_Q)\n",
    "            policy = policy / np.sum(policy)\n",
    "            action = np.random.choice(env_wrapper.discrete_actions, p=policy)\n",
    "            mu_learner += weight * env_wrapper.get_features(state)\n",
    "            state, _, done, _ = env_wrapper.step(action)\n",
    "            weight *= gamma\n",
    "\n",
    "    return mu_learner / n_trajs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6dffa80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================\n",
    "# 5. MaxEnt IRL Main Loop (FIXED)\n",
    "# =============================================\n",
    "\n",
    "def maxent_irl(expert_states, env_wrapper, lr=0.1, n_irl_iters=50, \n",
    "               n_vi_iters=100, n_trajs=10, n_bins=20):\n",
    "    feature_dim = len(env_wrapper.get_features(env_wrapper.reset()))\n",
    "    w = np.random.randn(feature_dim) * 0.1\n",
    "    mu_E = compute_expert_feature_expectations(expert_states)\n",
    "    losses = []\n",
    "\n",
    "    for it in tqdm(range(n_irl_iters)):\n",
    "        V, s_space = soft_value_iteration(w, env_wrapper, n_iters=n_vi_iters, n_bins=n_bins)\n",
    "        mu_learner = compute_learner_feature_expectations(w, env_wrapper, V, s_space, n_trajs=n_trajs)\n",
    "        gradient = mu_E - mu_learner\n",
    "        w += lr * gradient\n",
    "        loss = np.linalg.norm(gradient)\n",
    "        losses.append(loss)\n",
    "        tqdm.write(f\"Iter {it+1}/{n_irl_iters}, Loss: {loss:.4f}\")\n",
    "    \n",
    "    return w, losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90be149",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:37<05:35, 37.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/10, Loss: 4.3956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [01:15<05:02, 37.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 2/10, Loss: 4.3986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [01:52<04:23, 37.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 3/10, Loss: 3.9480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [02:30<03:45, 37.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 4/10, Loss: 7.3065\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =============================================\n",
    "# 6. Executionvec and Expert Data Handling (FIXED)\n",
    "# =============================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env_wrapper = CartPoleWrapper()\n",
    "\n",
    "    # Load expert data\n",
    "    with open(\"./expert_data/ckpt0.pkl\", \"rb\") as f:\n",
    "        exp_data = pickle.load(f)\n",
    "    exp_states = exp_data[\"states\"]\n",
    "    timestep_lens = exp_data[\"timestep_lens\"]\n",
    "\n",
    "    # Reconstruct expert state trajectories\n",
    "    expert_states = []\n",
    "    current = 0\n",
    "    for length in timestep_lens:\n",
    "        episode = exp_states[current:current+length]\n",
    "        expert_states.append(np.array(episode))\n",
    "        current += length\n",
    "\n",
    "    # Run MaxEnt IRL\n",
    "    learned_weights, losses = maxent_irl(\n",
    "        expert_states,\n",
    "        env_wrapper,\n",
    "        lr=0.1,\n",
    "        n_irl_iters=10,   # Keep low for initial testing\n",
    "        n_vi_iters=50,\n",
    "        n_trajs=5,\n",
    "        n_bins=10\n",
    "    )\n",
    "\n",
    "    print(\"Learned weights:\", learned_weights)\n",
    "    print(\"Losses:\", losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea3caaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_policy(w, env_wrapper, V, s_space, n_episodes=5):\n",
    "    total_rewards = []\n",
    "    for ep in range(n_episodes):\n",
    "        state = env_wrapper.reset()\n",
    "        done = False\n",
    "        ep_reward = 0.0\n",
    "        while not done:\n",
    "            # Discretize current state\n",
    "            state_idx = tuple(\n",
    "                np.clip(np.digitize(state[i], s_space[i]) - 1, 0, len(s_space[i]) - 1)\n",
    "                for i in range(env_wrapper.state_dim)\n",
    "            )\n",
    "\n",
    "            # Compute Q-values\n",
    "            Q_values = []\n",
    "            for a in env_wrapper.discrete_actions:\n",
    "                original_state = env_wrapper.env.env.state\n",
    "                env_wrapper.env.env.state = state\n",
    "                s_next, _, done, _ = env_wrapper.step(a)\n",
    "                s_next = np.array(s_next)\n",
    "                s_next_idx = tuple(\n",
    "                    np.clip(np.digitize(s_next[i], s_space[i]) - 1, 0, len(s_space[i]) - 1)\n",
    "                    for i in range(env_wrapper.state_dim)\n",
    "                )\n",
    "                cost = np.dot(w, env_wrapper.get_features(state))\n",
    "                Q = cost if done else cost + 0.99 * V[s_next_idx]\n",
    "                Q_values.append(Q)\n",
    "                env_wrapper.env.env.state = original_state\n",
    "\n",
    "            # Compute softmax policy\n",
    "            max_Q = np.max(Q_values)\n",
    "            policy = np.exp(Q_values - max_Q)\n",
    "            policy = policy / np.sum(policy)\n",
    "\n",
    "            # Sample action and step\n",
    "            action = np.random.choice(env_wrapper.discrete_actions, p=policy)\n",
    "            state, reward, done, _ = env_wrapper.step(action)\n",
    "            ep_reward += reward\n",
    "\n",
    "        total_rewards.append(ep_reward)\n",
    "        print(f\"Episode {ep+1}: Reward = {ep_reward:.2f}\")\n",
    "    \n",
    "    print(f\"\\nAverage reward over {n_episodes} episodes: {np.mean(total_rewards):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a6ffa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "V, s_space = soft_value_iteration(\n",
    "    learned_weights, env_wrapper, n_iters=50, n_bins=10\n",
    ")\n",
    "test_policy(learned_weights, env_wrapper, V, s_space, n_episodes=5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "act_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
